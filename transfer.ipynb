{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No change in resolution but varie the timestep\n",
    "semseg_transform = SemsegTransform()\n",
    "img_pil = semseg_transform.load('data/semseg_images_exercise_1')\n",
    "img_pil = semseg_transform.preprocess(img_pil)\n",
    "\n",
    "\n",
    "\n",
    "# verfiy the normalization\n",
    "plt.imshow(denormalize(img, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)[0].cpu().permute(1, 2, 0))\n",
    "\n",
    "res_list = [224, 256, 288, 320, 352, 384, 416, 448]\n",
    "timestep_list = [0,1, 10, 20, 50, 100]\n",
    "fig, axes = plt.subplots(len(res_list), len(timestep_list), figsize=(20, 10))\n",
    "\n",
    "for i,res in enumerate(res_list):\n",
    "    img_pil = center_crop(img_pil, (min(img_pil.size), min(img_pil.size))).resize((res,res))\n",
    "    img = semseg_transform.postprocess(img_pil).unsqueeze(0).to(device)\n",
    "    tokenized_semseg = toks['tok_semseg'].tokenize(img)\n",
    "    for j,t in enumerate(timestep_list):\n",
    "        if j == 0:\n",
    "            axes[i,j].imshow(denormalize(img, mean=IMAGENET_INCEPTION_STD, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "            axes[i,j].set_title(\"Original \")\n",
    "            axes[i,j].axis(\"off\")\n",
    "        else:\n",
    "            reconstructed_semseg = toks['tok_semseg'].decode_tokens(tokenized_semseg, image_size=res, timesteps=t)\n",
    "            axes[i,j].imshow(denormalize(reconstructed_semseg, mean=IMAGENET_INCEPTION_STD, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "            axes[i,j].set_title(f\"{res},{t}\")\n",
    "            axes[i,j].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 224 to Higher resolution and 448 to lower resolution\n",
    "semseg_transform = SemsegTransform()\n",
    "img_pil = semseg_transform.load('data/semseg_images_exercise_1')\n",
    "img_pil = semseg_transform.preprocess(img_pil)\n",
    "\n",
    "\n",
    "\n",
    "# verfiy the normalization\n",
    "plt.imshow(denormalize(img, mean=IMAGENET_INCEPTION_MEAN, std=IMAGENET_INCEPTION_STD)[0].cpu().permute(1, 2, 0))\n",
    "\n",
    "res_list_start = [244,448]\n",
    "res_list = [0,224, 256, 288, 320, 352, 384, 416, 448]\n",
    "fig, axes = plt.subplots(len(res_list_start),len(res_list), figsize=(20, 10))\n",
    "\n",
    "for i,res_start in enumerate(res_list_start):\n",
    "    img_pil = center_crop(img_pil, (min(img_pil.size), min(img_pil.size))).resize((res,res))\n",
    "    img = semseg_transform.postprocess(img_pil).unsqueeze(0).to(device)\n",
    "    tokenized_semseg = toks['tok_semseg'].tokenize(img)\n",
    "    for j,res in enumerate(res_list):\n",
    "        if i == 0:\n",
    "            axes[i,j].imshow(denormalize(img, mean=IMAGENET_INCEPTION_STD, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "            axes[i,j].set_title(\"Original \")\n",
    "            axes[i,j].axis(\"off\")\n",
    "        else:\n",
    "            reconstructed_semseg = toks['tok_semseg'].decode_tokens(tokenized_semseg, image_size=res, timesteps=20)\n",
    "            axes[i,j].imshow(denormalize(reconstructed_semseg, mean=IMAGENET_INCEPTION_STD, std=IMAGENET_INCEPTION_STD)[0].permute(1, 2, 0).cpu())\n",
    "            axes[i,j].set_title(f\"{res},{t}\")\n",
    "            axes[i,j].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tok_semseg(rgb_img, mod_dict, tokenizers, key='tok_semseg', image_size=224, patch_size=16, use_detectron=True, return_logits=False):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of semantic segmentation tokens from a model dictionary into an RGB image.\n",
    "\n",
    "    Args:\n",
    "        rgb_img (torch.Tensor): RGB image to overlay the semantic segmentation on. # What is this RGB Image => Like a container where the semseg will be transfered\n",
    "        mod_dict (dict): Model output dictionary.                                  # What is the model output dictionary?\n",
    "        tokenizers (dict): Dictionary of tokenizers.\n",
    "        key (str): Key of the tokenized semantic segmentation modality to decode.\n",
    "        image_size (int): Size of the image.\n",
    "        patch_size (int): Size of the patches.\n",
    "        use_detectron (bool): Uses detectron2's visualization for the semseg output.\n",
    "    \"\"\"\n",
    "    tokens = mod_dict[key]['tensor']\n",
    "    tokens = tokens.unsqueeze(0) if tokens.ndim == 1 else tokens\n",
    "    img_tok = rearrange(tokens, \"b (nh nw) -> b nh nw\", nh=image_size//patch_size, nw=image_size//patch_size)\n",
    "    rec = tokenizers[get_transform_key(key)].decode_tokens(img_tok).detach().cpu()\n",
    "    if return_logits:\n",
    "        return rec\n",
    "    semsegs = rec.argmax(1)\n",
    "    B, H, W = semsegs.shape\n",
    "\n",
    "    if not use_detectron:\n",
    "        return semsegs if B > 1 else semsegs[0]\n",
    "    else:\n",
    "        rgb_imgs = [rgb_img] * B\n",
    "        imgs = []\n",
    "        for rgb, semseg in zip(rgb_imgs, semsegs):\n",
    "            if USE_DETECTRON:\n",
    "                v = Visualizer(255*rgb, coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)\n",
    "                img = v.draw_sem_seg((semseg-1).cpu()).get_image() / 255.0\n",
    "            else:\n",
    "                colormap = plt.get_cmap('viridis')\n",
    "                img = colormap(semseg.cpu())[..., :3]\n",
    "            imgs.append(img)\n",
    "        imgs = np_squeeze(np.stack(imgs), axis=0)\n",
    "        return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_transform = RGBTransform(imagenet_default_mean_and_std=False)\n",
    "\n",
    "# Sample the video => Return Numpy array N x H x W x C where N = step \n",
    "video_path = 'data/videos_exercise_1'\n",
    "sampled_frames = load_sampled_frames(video_path, step=10)\n",
    "\n",
    "# Transform the array to a tensor ( Same memory as numpy array = tensor not resizable)\n",
    "sample_tensor = torch.from_numpy(sampled_frames) \n",
    "\n",
    "for idx in range(sample_tensor[0]):\n",
    "    sample = sample_tensor[idx,:,:,:]\n",
    "    # Print Each frame \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
